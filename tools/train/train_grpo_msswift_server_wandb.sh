#!/usr/bin/env bash
set -euo pipefail

# Convenience wrapper: GRPO training on GPU0 + external vLLM server + W&B.
# Start a vLLM server separately (e.g., on GPU1) before running this script.
#
# Defaults (override via env vars):
#   CUDA_VISIBLE_DEVICES=0
#   DATASET=hf::Hi-ToM/Hi-ToM_Dataset
#   OUTPUT_DIR=output/qwen3-8b-hitom-grpo
#   NUM_GENERATIONS=4
#   GENERATION_BATCH_SIZE=4
#   VLLM_SERVER_HOST=127.0.0.1
#   VLLM_SERVER_PORT=8001
#   REPORT_TO=wandb
#   RUN_NAME=grpo-qwen3-hitom
#   WANDB_PROJECT=mindgames

CUDA_VISIBLE_DEVICES="${CUDA_VISIBLE_DEVICES:-0}"
DATASET="${DATASET:-hf::Hi-ToM/Hi-ToM_Dataset}"
OUTPUT_DIR="${OUTPUT_DIR:-output/qwen3-8b-hitom-grpo}"
NUM_GENERATIONS="${NUM_GENERATIONS:-4}"
GENERATION_BATCH_SIZE="${GENERATION_BATCH_SIZE:-$NUM_GENERATIONS}"
VLLM_SERVER_HOST="${VLLM_SERVER_HOST:-127.0.0.1}"
VLLM_SERVER_PORT="${VLLM_SERVER_PORT:-8001}"
REPORT_TO="${REPORT_TO:-wandb}"
RUN_NAME="${RUN_NAME:-grpo-qwen3-hitom}"
WANDB_PROJECT="${WANDB_PROJECT:-mindgames}"

REPORT_TO="$REPORT_TO" RUN_NAME="$RUN_NAME" WANDB_PROJECT="$WANDB_PROJECT" \
CUDA_VISIBLE_DEVICES="$CUDA_VISIBLE_DEVICES" VLLM_MODE=server \
VLLM_SERVER_HOST="$VLLM_SERVER_HOST" VLLM_SERVER_PORT="$VLLM_SERVER_PORT" \
DATASET="$DATASET" OUTPUT_DIR="$OUTPUT_DIR" \
NUM_GENERATIONS="$NUM_GENERATIONS" GENERATION_BATCH_SIZE="$GENERATION_BATCH_SIZE" \
bash tools/train/train_grpo_msswift.sh
